import streamlit as st
from langchain_openai import OpenAI
import os
from dotenv import load_dotenv
from stt.whisper import whisper_transcribe_from_file
import wave
import io
from st_audiorec import st_audiorec
from streamlit_chat import message
    
# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(override=True)

# OpenAI API í‚¤ ì„¤ì •
llm = OpenAI()

# ì±—ë´‡ ì‘ë‹µ ìƒì„± í•¨ìˆ˜

def chatbot_response(user_input, chat_history):
    messages = [{"role": "system", "content": "ë„ˆëŠ” ì¹´í˜ì—ì„œ ê³ ê°ì˜ ì£¼ë¬¸ì„ ë„ì™€ì£¼ëŠ” ì±—ë´‡ì´ì•¼. ìµœëŒ€í•œ ì¹œì ˆí•˜ê²Œ ì£¼ë¬¸ì„ ë°›ì•„ì¤˜."}]
    for entry in chat_history:
        messages.append({"role": "user", "content": entry["user"]})
        messages.append({"role": "assistant", "content": entry["bot"]})

    messages.append({"role": "user", "content": user_input})

    response = llm.invoke(user_input)

    return response

# Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •
st.set_page_config(page_title="Chatbot Interface", layout="wide")

st.title("TalkASK ğŸ“¢ ")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = [{"user":'', "bot": "ì–´ì„œì˜¤ì„¸ìš”. TalkAskì…ë‹ˆë‹¤ ğŸ˜Š ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? "}]

if "filtered_images" not in st.session_state:
    st.session_state.filtered_images = []

# ì´ë¯¸ì§€ ë°ì´í„° (ì˜ˆì‹œ)
image_data = {
    "category1": ["images/menu1.jpg", "images/menu2.jpg"],
    "category2": ["images/menu3.jpg", "images/menu4.jpg"],
    "category3": ["images/menu5.jpg", "images/menu6.jpg"]
}

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ì½œë°± í•¨ìˆ˜
def handle_input(input):
    user_input = input
    if user_input:
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.chat_history.append({"user": user_input, "bot": ""})
        
        # ì±—ë´‡ ì‘ë‹µ ìƒì„±
        bot_response = chatbot_response(user_input, st.session_state.chat_history)
        
        # ì±—ë´‡ ì‘ë‹µ ì¶”ê°€
        st.session_state.chat_history[-1]["bot"] = bot_response
        
        # ì‚¬ìš©ìì˜ ì…ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ í•„í„°ë§ (ì˜ˆì‹œë¡œ ê°„ë‹¨í•œ í‚¤ì›Œë“œ í•„í„°ë§ ì‚¬ìš©)
        if "category1" in user_input:
            st.session_state.filtered_images = image_data["category1"]
        elif "category2" in user_input:
            st.session_state.filtered_images = image_data["category2"]
        elif "category3" in user_input:
            st.session_state.filtered_images = image_data["category3"]
        else:
            st.session_state.filtered_images = []

        # ì…ë ¥ í•„ë“œ ì´ˆê¸°í™”
        st.session_state.user_input = ""

# ë‘ ê°œì˜ ì—´ ìƒì„±
col1, col2 = st.columns([1, 3])

with col1:
    
    st.markdown("<h3 style='text-align: center;'>Talk Ask ChatbotğŸ¤– </h3>", unsafe_allow_html=True)
    # ì±„íŒ… ì…ë ¥
    st.session_state.user_input = st_audiorec()
    
    def get_wav_info(byte_data):
        """
        WAV ë°”ì´íŠ¸ ë°ì´í„°ì—ì„œ ì˜¤ë””ì˜¤ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        with io.BytesIO(byte_data) as wav_buffer:
            with wave.open(wav_buffer, 'rb') as wav_file:
                channels = wav_file.getnchannels()
                sample_width = wav_file.getsampwidth()
                frame_rate = wav_file.getframerate()
                frames = wav_file.readframes(wav_file.getnframes())
        return channels, sample_width, frame_rate, frames

    # byte íŒŒì¼ -> wav íŒŒì¼ë¡œ ë³€í™˜
    def bytes_to_wav(byte_data, output_filename):
        """
        ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ WAV íŒŒì¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        """
        channels, sample_width, frame_rate, frames = get_wav_info(byte_data)
        
        with wave.open(output_filename, 'wb') as wav_file:
            wav_file.setnchannels(channels)
            wav_file.setsampwidth(sample_width)
            wav_file.setframerate(frame_rate)
            wav_file.writeframes(frames)

    # wav íŒŒì¼ ì €ì¥ í›„ STT ì‹¤í–‰í•˜ê¸°
    if st.button("ì±„íŒ… ë³´ë‚´ê¸°", key = 'ss'):
        bytes_to_wav(st.session_state.user_input, 'output.wav')
        handle_input(whisper_transcribe_from_file('output.wav', 'ìˆ«ìì„¸ê¸°'))

    # ì±„íŒ… ë‚´ì—­ í‘œì‹œ
    for i, entry in enumerate(st.session_state.chat_history):
        if entry['user']:  # ì‚¬ìš©ì ë©”ì‹œì§€ê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ í‘œì‹œ
            message(entry['user'], is_user=True, key=f"user_{i}")
        message(entry['bot'], key=f"bot_{i}")


with col2:
    st.title("ë©”ë‰´íŒ")
    # í•„í„°ë§ëœ ì´ë¯¸ì§€ í‘œì‹œ
    for image_path in st.session_state.filtered_images:
        st.image(image_path, use_column_width=True)

# ìŠ¤íƒ€ì¼ ì ìš©
st.markdown("""
<style>
    .chat-container {
        background-color: #DFF0D8; /* ì±„íŒ… ì»¨í…Œì´ë„ˆ ë°°ê²½ìƒ‰ */
        padding: 10px;
        border-radius: 10px;
    }
    .image-container {
        background-color: #FFFFFF; /* ì´ë¯¸ì§€ ì»¨í…Œì´ë„ˆ ë°°ê²½ìƒ‰ */
        padding: 10px;
        border-radius: 10px;
    }
    .user-message {
        background-color: #dcf8c6;
        padding: 10px;
        border-radius: 10px;
        margin: 5px 0;
        text-align: right;
    }
    .chatbot-message {
        background-color: #f1f1f1;
        padding: 10px;
        border-radius: 10px;
        margin: 5px 0;
        text-align: left;
    }
    .responsive-image img {
        width: 100%;
        height: auto;
    }
    .stTextInput>div>input {
        width: 100%;
    }
    .stButton>button {
        width: 100%;
    }
    .stColumn:nth-child(1) {
        background-color: #DFF0D8; /* ì²« ë²ˆì§¸ ì»¬ëŸ¼ ë°°ê²½ìƒ‰ */
        padding: 10px;
        border-radius: 10px;
    }
    .stColumn:nth-child(2) {
        background-color: #FFFFFF; /* ë‘ ë²ˆì§¸ ì»¬ëŸ¼ ë°°ê²½ìƒ‰ */
        padding: 10px;
        border-radius: 10px;
    }
</style>
""", unsafe_allow_html=True)